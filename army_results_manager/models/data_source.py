from odoo import models, fields, api
import requests
from bs4 import BeautifulSoup
import logging

_logger = logging.getLogger(__name__)

class DataSource(models.Model):
    _name = 'data.source'
    _description = 'Nguá»“n thu tháº­p thÃ´ng tin'

    name = fields.Char(string='TÃªn nguá»“n', required=True)
    source_type = fields.Selection([
        ('rss', 'RSS Feed'),
        ('api', 'Webservice (API)'),
        ('html', 'Trang web (HTML)'),
    ], string="PhÆ°Æ¡ng thá»©c", required=True)
    category = fields.Selection([
        ('tin_tuc', 'Tin tá»©c'),
        ('su_kien', 'Sá»± kiá»‡n'),
        ('thong_bao', 'ThÃ´ng bÃ¡o'),
    ], string='Loáº¡i tin cáº§n thu tháº­p', required=True)
    url = fields.Char(string='URL', required=True)
    active = fields.Boolean(string='KÃ­ch hoáº¡t', default=True)
    last_collected = fields.Datetime(string='Láº§n thu tháº­p gáº§n nháº¥t', readonly=True)

    def action_collect_now(self):
        self._collect_from_source()

    def _collect_from_source(self):
        """Thu tháº­p dá»¯ liá»‡u tá»« nguá»“n"""
        for record in self:
            try:
                response = requests.get(record.url, timeout=10)
                response.raise_for_status()

                soup = BeautifulSoup(response.text, "html.parser")
                articles = soup.find_all("article")

                results = []
                for art in articles:
                    a_tag = art.find("a", href=True, title=True)
                    title = a_tag["title"].strip() if a_tag else ""
                    link = a_tag["href"].strip() if a_tag else ""

                    # Láº¥y áº£nh trong bÃ i
                    img_tag = art.find("img")
                    img_src = img_tag["src"].strip() if img_tag and img_tag.has_attr("src") else ""

                    if not img_src or not link:
                        continue

                    # ðŸ”¹ Láº¥y toÃ n bá»™ <p> trong trang chi tiáº¿t bÃ i viáº¿t
                    description = ""
                    try:
                        article_page = requests.get(link, timeout=10)
                        article_page.raise_for_status()
                        article_soup = BeautifulSoup(article_page.text, "html.parser")

                        # Gá»™p táº¥t cáº£ cÃ¡c tháº» <p> trong ná»™i dung bÃ i
                        paragraphs = article_soup.find_all("p")
                        description = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
                    except Exception as sub_e:
                        _logger.warning(f"KhÃ´ng thá»ƒ láº¥y ná»™i dung chi tiáº¿t tá»« {link}: {sub_e}")

                    results.append({
                        "title": title,
                        "source_id": self.id,
                        "link": link,
                        "image_url": img_src,
                        "description": description,
                        "category": "tin_tuc",
                    })

                Collected = self.env['collected.data']
                Collected.create(results)
            except Exception as e:
                _logger.error(f"Lá»—i khi thu tháº­p dá»¯ liá»‡u tá»« {record.url}: {e}")
